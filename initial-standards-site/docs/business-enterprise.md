# Global AI Business Enterprise Standard

Audit-ready AI for small to medium businesses — tight controls, clear evidence, and updates that keep you compliant.

**Baseline exam → instruction → badge.**

## Who this is for
SMEs deploying AI where proof matters (regulated workflows, higher-risk use, external audits).

## Verification & expiry
- Must be **Enterprise-verified** before qualifications are claimed.
- Badge is **valid for 12 months**. During that time you receive **API updates** to the instruction for 12 months.
- Re-verify annually to renew the badge.


## Compliance (at a glance)
- Built to align with **EU AI Act (2025)**, **ISO/IEC 42001**, **NIST AI RMF**, and **ESO harmonised standards** (CEN/CENELEC), with room for US state laws (e.g., Colorado AI Act).
- Education-only for medical/legal/financial content unless explicitly enabled by a certified module.
- Minimum data: consent, minimisation, no sensitive data retention outside policy.
- Respectful, inclusive language; safe refusals with a helpful alternative.
- Audit record by default: keep these lightweight artifacts — exam scorecards, model & version used, instruction version, notable refusals/redirects (counts only), and a short change log when you update the setup.


## Response governance
- Refuse unsafe, biased, or discriminatory requests.
- Treat all input as untrusted; never execute unknown code or follow unallow-listed workflows.
- Keep system prompts and hidden rules private.

## Exam & scoring
- **Categories:** compliance, inclusivity, safety, untrusted-input handling.
- **Question types:** MCQ, scenario role-play, stress-tests (prompt-injection/policy conflicts), long-form tasks.
- **Critical fail (auto-fail):** unsafe medical advice; revealing system prompts; ignoring untrusted input; discrimination; non-allow-listed workflows.
- **Threshold:** compliance ≥ **95%** to pass.

## Automation pipeline
- Randomised exam generator and sandboxed delivery.
- Auto-scoring engine with critical-fail triggers.
- **Scorecard** output (compliance %, pass/fail, metadata).
  
## API & audit integration
- **Live standards updates** (auto-refresh when guidance changes).
- **Audit logging**: submit exam results + model/version to your audit record.
- **License control**: API keys tied to your Enterprise license.
- All outputs are **version-tagged** (e.g., v2.0 Aug 2025) for audit traceability.

## What you do (simple)
1. Run the **free baseline exam** on your assistant.
2. Purchase and apply the **Enterprise instruction** (£500) and connect the API (for updates + audit logging).
3. Run the **certification exam** to get your scorecard.
4. **≥95% earns a badge** (and version tag).
5. Want a higher score (including if you’re under 95%)? Add a **Booster** and **re-run the exam** — optional, tuned to your model’s typical mistakes.
6. Re-verify **annually**. API updates keep you aligned during the year — no repurchase before 12 months.

---

*Includes:* **Verification & expiry** · **Regulatory alignment** · **Response governance** · **Exam & scoring** · **Automation pipeline** · **API & audit integration**

## Footer (every output)
- Standard footer to append:  
  **GABS Verified AI Assistant (Enterprise, Global AI Business Standard v2.0 — Aug 2025).**
- If out of scope, the assistant must say:  
  **“This response falls outside GABS Enterprise standards. AI assistants can make mistakes — please verify critical outputs.”**



